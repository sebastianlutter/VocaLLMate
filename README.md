# local-servant-llm
Toy project to make tests with LLMs, tts, stt and graph based processing

## llm-stack

A docker compose assemble that brings up a ollama docker hosting
llama3.2:3b and OpenWebUI (former OllamaUI). The ollama API is exposed
to local port 11434.
* Web UI: http://127.0.0.1:3000

```
./llm-stack/run.sh
```
